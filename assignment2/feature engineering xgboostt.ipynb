{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from cleaned.csv: 9078 rows, 36 columns\n",
      "Data loaded from test_set_VU_DM.csv: 9078 rows, 50 columns\n",
      "\n",
      "Random sample from cleaned.csv:\n",
      "      visitor_hist_starrating  visitor_hist_adr_usd  prop_location_score1  \\\n",
      "1400                 0.024487             -0.048958             -0.707107   \n",
      "8074                -6.027945             -1.434038             -0.602601   \n",
      "743                  0.024487             -0.048958              0.840890   \n",
      "1488                 0.024487             -0.048958              0.044031   \n",
      "267                  0.024487             -0.048958             -0.151918   \n",
      "\n",
      "      prop_location_score2  prop_starrating  price_usd  gross_bookings_usd  \\\n",
      "1400             -0.334591        -0.171762  -0.009512           -0.033481   \n",
      "8074             -0.799936        -0.171762  -0.009012           -0.033481   \n",
      "743               0.812702         1.731145  -0.006366           -0.033481   \n",
      "1488             -0.556084         0.779692  -0.002230           -0.033481   \n",
      "267              -0.493898         0.779692  -0.007200           -0.033481   \n",
      "\n",
      "      position  comp1_rate  comp1_inv  ...  comp7_rate  comp7_inv  \\\n",
      "1400  1.164796    0.098425  -0.021666  ...   -0.061847  -0.068279   \n",
      "8074 -1.233135    0.098425  -0.021666  ...   -0.061847  -0.068279   \n",
      "743  -1.424969    0.098425  -0.021666  ...   -0.061847  -0.068279   \n",
      "1488 -0.753549    0.098425  -0.021666  ...   -0.061847  -0.068279   \n",
      "267  -0.849466    0.098425  -0.021666  ...   -0.061847  -0.068279   \n",
      "\n",
      "      comp7_rate_percent_diff  comp8_rate  comp8_inv  comp8_rate_percent_diff  \\\n",
      "1400                -0.022646    0.080288  -7.809438                -0.004492   \n",
      "8074                -0.022646    0.080288  -0.031063                -0.004492   \n",
      "743                 -0.022646    0.080288  -0.031063                -0.004492   \n",
      "1488                -0.022646    0.080288   7.747312                -0.004492   \n",
      "267                 -0.022646    0.080288  -0.031063                -0.004492   \n",
      "\n",
      "      click_bool  booking_bool  srch_id            date_time  \n",
      "1400         0.0           0.0     99.0  2013-02-18 16:22:13  \n",
      "8074         0.0           0.0    569.0  2013-02-26 12:43:08  \n",
      "743          1.0           0.0     63.0  2013-05-23 11:56:25  \n",
      "1488         0.0           0.0    103.0  2013-02-19 08:44:43  \n",
      "267          0.0           0.0     28.0  2012-11-26 10:13:17  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "\n",
      "Summary statistics of cleaned.csv:\n",
      "        visitor_hist_starrating  visitor_hist_adr_usd  prop_location_score1  \\\n",
      "count               9078.000000           9078.000000           9078.000000   \n",
      "unique                      NaN                   NaN                   NaN   \n",
      "top                         NaN                   NaN                   NaN   \n",
      "freq                        NaN                   NaN                   NaN   \n",
      "mean                  -0.041349              0.005088             -0.011482   \n",
      "std                    0.916024              0.641330              1.008201   \n",
      "min                   -7.302141             -3.173759             -1.876269   \n",
      "25%                    0.024487             -0.048958             -0.707107   \n",
      "50%                    0.024487             -0.048958             -0.106197   \n",
      "75%                    0.024487             -0.048958              0.729852   \n",
      "max                    9.899508              9.566715              2.682810   \n",
      "\n",
      "        prop_location_score2  prop_starrating    price_usd  \\\n",
      "count            9078.000000      9078.000000  9078.000000   \n",
      "unique                   NaN              NaN          NaN   \n",
      "top                      NaN              NaN          NaN   \n",
      "freq                     NaN              NaN          NaN   \n",
      "mean               -0.009606        -0.021256    -0.006233   \n",
      "std                 1.000519         1.010414     0.007976   \n",
      "min                -0.816705        -3.026122    -0.015009   \n",
      "25%                -0.621589        -0.171762    -0.010564   \n",
      "50%                -0.334591        -0.171762    -0.008387   \n",
      "75%                 0.108221         0.779692    -0.004514   \n",
      "max                 6.169763         1.731145     0.174723   \n",
      "\n",
      "        gross_bookings_usd     position   comp1_rate    comp1_inv  ...  \\\n",
      "count          9078.000000  9078.000000  9078.000000  9078.000000  ...   \n",
      "unique                 NaN          NaN          NaN          NaN  ...   \n",
      "top                    NaN          NaN          NaN          NaN  ...   \n",
      "freq                   NaN          NaN          NaN          NaN  ...   \n",
      "mean              0.006560    -0.011336    -0.008423     0.007742  ...   \n",
      "std               0.707856     1.006412     1.096344     0.970255  ...   \n",
      "min              -1.404967    -1.520886   -15.546189   -26.718869  ...   \n",
      "25%              -0.033481    -0.849466     0.098425    -0.021666  ...   \n",
      "50%              -0.033481    -0.178045     0.098425    -0.021666  ...   \n",
      "75%              -0.033481     0.877045     0.098425    -0.021666  ...   \n",
      "max              30.497899     2.123969     0.098425    26.675536  ...   \n",
      "\n",
      "         comp7_rate    comp7_inv  comp7_rate_percent_diff   comp8_rate  \\\n",
      "count   9078.000000  9078.000000              9078.000000  9078.000000   \n",
      "unique          NaN          NaN                      NaN          NaN   \n",
      "top             NaN          NaN                      NaN          NaN   \n",
      "freq            NaN          NaN                      NaN          NaN   \n",
      "mean       0.035023     0.031070                 0.018800    -0.005372   \n",
      "std        1.123347     1.223840                 1.044177     1.019005   \n",
      "min       -6.723892   -11.484633                -1.113208    -3.330318   \n",
      "25%       -0.061847    -0.068279                -0.022646     0.080288   \n",
      "50%       -0.061847    -0.068279                -0.022646     0.080288   \n",
      "75%       -0.061847    -0.068279                -0.022646     0.080288   \n",
      "max        6.600197    11.348075                56.141322     3.490894   \n",
      "\n",
      "          comp8_inv  comp8_rate_percent_diff   click_bool  booking_bool  \\\n",
      "count   9078.000000              9078.000000  9078.000000   9078.000000   \n",
      "unique          NaN                      NaN          NaN           NaN   \n",
      "top             NaN                      NaN          NaN           NaN   \n",
      "freq            NaN                      NaN          NaN           NaN   \n",
      "mean      -0.002787                -0.003245     0.045495      0.028090   \n",
      "std        1.002845                 0.014475     0.208398      0.165239   \n",
      "min       -7.809438                -0.033018     0.000000      0.000000   \n",
      "25%       -0.031063                -0.004492     0.000000      0.000000   \n",
      "50%       -0.031063                -0.004492     0.000000      0.000000   \n",
      "75%       -0.031063                -0.004492     0.000000      0.000000   \n",
      "max        7.747312                 0.445591     1.000000      1.000000   \n",
      "\n",
      "            srch_id            date_time  \n",
      "count   9078.000000                 9078  \n",
      "unique          NaN                  375  \n",
      "top             NaN  2013-06-25 15:39:35  \n",
      "freq            NaN                   36  \n",
      "mean     321.593413                  NaN  \n",
      "std      186.307558                  NaN  \n",
      "min        1.000000                  NaN  \n",
      "25%      147.000000                  NaN  \n",
      "50%      328.000000                  NaN  \n",
      "75%      484.000000                  NaN  \n",
      "max      634.000000                  NaN  \n",
      "\n",
      "[11 rows x 36 columns]\n",
      "\n",
      "Data Types and Memory Usage:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9078 entries, 0 to 9077\n",
      "Data columns (total 36 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   visitor_hist_starrating  9078 non-null   float64\n",
      " 1   visitor_hist_adr_usd     9078 non-null   float64\n",
      " 2   prop_location_score1     9078 non-null   float64\n",
      " 3   prop_location_score2     9078 non-null   float64\n",
      " 4   prop_starrating          9078 non-null   float64\n",
      " 5   price_usd                9078 non-null   float64\n",
      " 6   gross_bookings_usd       9078 non-null   float64\n",
      " 7   position                 9078 non-null   float64\n",
      " 8   comp1_rate               9078 non-null   float64\n",
      " 9   comp1_inv                9078 non-null   float64\n",
      " 10  comp1_rate_percent_diff  9078 non-null   float64\n",
      " 11  comp2_rate               9078 non-null   float64\n",
      " 12  comp2_inv                9078 non-null   float64\n",
      " 13  comp2_rate_percent_diff  9078 non-null   float64\n",
      " 14  comp3_rate               9078 non-null   float64\n",
      " 15  comp3_inv                9078 non-null   float64\n",
      " 16  comp3_rate_percent_diff  9078 non-null   float64\n",
      " 17  comp4_rate               9078 non-null   float64\n",
      " 18  comp4_inv                9078 non-null   float64\n",
      " 19  comp4_rate_percent_diff  9078 non-null   float64\n",
      " 20  comp5_rate               9078 non-null   float64\n",
      " 21  comp5_inv                9078 non-null   float64\n",
      " 22  comp5_rate_percent_diff  9078 non-null   float64\n",
      " 23  comp6_rate               9078 non-null   float64\n",
      " 24  comp6_inv                9078 non-null   float64\n",
      " 25  comp6_rate_percent_diff  9078 non-null   float64\n",
      " 26  comp7_rate               9078 non-null   float64\n",
      " 27  comp7_inv                9078 non-null   float64\n",
      " 28  comp7_rate_percent_diff  9078 non-null   float64\n",
      " 29  comp8_rate               9078 non-null   float64\n",
      " 30  comp8_inv                9078 non-null   float64\n",
      " 31  comp8_rate_percent_diff  9078 non-null   float64\n",
      " 32  click_bool               9078 non-null   float64\n",
      " 33  booking_bool             9078 non-null   float64\n",
      " 34  srch_id                  9078 non-null   float64\n",
      " 35  date_time                9078 non-null   object \n",
      "dtypes: float64(35), object(1)\n",
      "memory usage: 3.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "sample_size = random.randint(9000, 10000)\n",
    "\n",
    "data_path = 'cleaned.csv'\n",
    "test_data_path = 'test_set_VU_DM.csv'\n",
    "\n",
    "df = pd.read_csv(data_path, nrows=sample_size)\n",
    "df_test = pd.read_csv(test_data_path, nrows=sample_size)\n",
    "\n",
    "print(f\"Data loaded from {data_path}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"Data loaded from {test_data_path}: {df_test.shape[0]} rows, {df_test.shape[1]} columns\")\n",
    "\n",
    "print(\"\\nRandom sample from cleaned.csv:\")\n",
    "print(df.sample(n=5, random_state=42))\n",
    "\n",
    "print(\"\\nSummary statistics of cleaned.csv:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "print(\"\\nData Types and Memory Usage:\")\n",
    "memory_info = df.info(memory_usage='deep')\n",
    "print(memory_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['visitor_hist_starrating', 'visitor_hist_adr_usd',\n",
       "       'prop_location_score1', 'prop_location_score2', 'prop_starrating',\n",
       "       'price_usd', 'gross_bookings_usd', 'position', 'comp1_rate',\n",
       "       'comp1_inv', 'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
       "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
       "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
       "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
       "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
       "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
       "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
       "       'comp8_rate_percent_diff', 'click_bool', 'booking_bool', 'srch_id',\n",
       "       'date_time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   7,  10,  13,  14,  16,  18,  19,  23,  24,  27,  32,  33,\n",
       "        34,  35,  51,  54,  55,  58,  62,  68,  69,  70,  76,  77,  86,\n",
       "        88,  89,  91,  95,  98, 100, 130, 132, 137, 150, 156, 164, 169,\n",
       "       171, 173, 182, 186, 197, 200, 206, 209, 212, 215, 227, 228, 232,\n",
       "       233, 238, 239, 245, 254, 255, 256, 258, 261, 269, 270, 272, 274,\n",
       "       278, 279, 280, 283, 284, 285, 286, 287, 291, 298, 302, 304, 315,\n",
       "       318, 319, 323, 327, 334, 341, 347, 349, 352, 358, 363, 364, 368,\n",
       "       369, 373, 378, 380, 381, 389, 393, 403, 404, 414, 415, 417, 420,\n",
       "       421, 422, 426, 437, 440, 443, 446, 448, 450, 454, 459, 464, 467,\n",
       "       481, 482, 485, 487, 488, 492, 493, 494, 495, 496, 499, 504, 505,\n",
       "       506, 515, 519, 525, 526, 538, 541, 542, 544, 546, 550, 552, 554,\n",
       "       561, 572, 577])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_srch_id_test = np.unique(df_test['srch_id'])\n",
    "unique_srch_id_train = np.unique(df['srch_id'])\n",
    "\n",
    "missing_srch_id = unique_srch_id_test[~np.isin(unique_srch_id_test, unique_srch_id_train)]\n",
    "missing_srch_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['visitor_hist_starrating', 'visitor_hist_adr_usd',\n",
       "       'prop_location_score1', 'prop_location_score2', 'prop_starrating',\n",
       "       'price_usd', 'gross_bookings_usd', 'position', 'comp1_rate',\n",
       "       'comp1_inv', 'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
       "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
       "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
       "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
       "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
       "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
       "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
       "       'comp8_rate_percent_diff', 'click_bool', 'booking_bool', 'srch_id',\n",
       "       'date_time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data transformation complete. Statistics:\n",
      "          position   click_bool  booking_bool  gross_bookings_usd       target\n",
      "count  9078.000000  9078.000000   9078.000000         9078.000000  9078.000000\n",
      "mean     -0.125593     0.045495      0.028090            0.040041     0.157854\n",
      "std       1.371345     0.208398      0.165239            0.707856     0.833553\n",
      "min      -5.199338     0.000000      0.000000           -1.371486     0.000000\n",
      "25%      -0.619855     0.000000      0.000000            0.000000     0.000000\n",
      "50%      -0.037988     0.000000      0.000000            0.000000     0.000000\n",
      "75%       0.698526     0.000000      0.000000            0.000000     0.000000\n",
      "max       5.199338     1.000000      1.000000           30.531380     5.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "import numpy as np\n",
    "\n",
    "df['target'] = np.select(\n",
    "    [df['booking_bool'] == 1, (df['booking_bool'] == 0) & (df['click_bool'] == 1)],\n",
    "    [5, 1],\n",
    "    default=0\n",
    ")\n",
    "\n",
    "scaler = RobustScaler()\n",
    "rank_scaler = QuantileTransformer(n_quantiles=100, output_distribution='normal')\n",
    "\n",
    "df['position'] = rank_scaler.fit_transform(df[['position']])\n",
    "df[['click_bool', 'booking_bool', 'gross_bookings_usd']] = scaler.fit_transform(\n",
    "    df[['click_bool', 'booking_bool', 'gross_bookings_usd']]\n",
    ")\n",
    "\n",
    "df['target'] = (4 * df['booking_bool'] + df['click_bool']).round(2)\n",
    "\n",
    "print(\"Data transformation complete. Statistics:\")\n",
    "print(df[['position', 'click_bool', 'booking_bool', 'gross_bookings_usd', 'target']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_feature_engineering(df):\n",
    "    df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day_of_week'] = df['date_time'].dt.dayofweek\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "    count_cols = ['positive_rate_count', 'negative_rate_count', \n",
    "                  'positive_inv_count', 'negative_inv_count', \n",
    "                  'positive_rate_diff_count', 'negative_rate_diff_count']\n",
    "    df[count_cols] = 0\n",
    "\n",
    "    comp_rate_cols = [f'comp{i}_rate' for i in range(1, 9)]\n",
    "    comp_inv_cols = [f'comp{i}_inv' for i in range(1, 9)]\n",
    "    comp_rate_percent_diff_cols = [f'comp{i}_rate_percent_diff' for i in range(1, 9)]\n",
    "\n",
    "    df['avg_rate'] = df[comp_rate_cols].mean(axis=1)\n",
    "    df['avg_inv'] = df[comp_inv_cols].mean(axis=1)\n",
    "    df['avg_rate_percent_diff'] = df[comp_rate_percent_diff_cols].mean(axis=1)\n",
    "\n",
    "    df['positive_rate_count'] = df[comp_rate_cols].gt(0).sum(axis=1)\n",
    "    df['negative_rate_count'] = df[comp_rate_cols].lt(0).sum(axis=1)\n",
    "\n",
    "    df['positive_inv_count'] = df[comp_inv_cols].gt(0).sum(axis=1)\n",
    "    df['negative_inv_count'] = df[comp_inv_cols].lt(0).sum(axis=1)\n",
    "\n",
    "    df['positive_rate_diff_count'] = df[comp_rate_percent_diff_cols].gt(0).sum(axis=1)\n",
    "    df['negative_rate_diff_count'] = df[comp_rate_percent_diff_cols].lt(0).sum(axis=1)\n",
    "\n",
    "    df['location_score'] = df[['prop_location_score1', 'prop_location_score2']].mean(axis=1)\n",
    "    \n",
    "    df['star_rating_diff'] = df['visitor_hist_starrating'].sub(df['prop_starrating'])\n",
    "    df['price_diff'] = df['visitor_hist_adr_usd'].sub(df['price_usd'])\n",
    "    \n",
    "    df['price_location_interaction'] = df['price_usd'] * df['location_score']\n",
    "\n",
    "    df.drop(columns=comp_rate_cols + comp_inv_cols, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = complete_feature_engineering(df)\n",
    "df_test = complete_feature_engineering(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def normalize_features(input_df, group_key, target_column, take_log10=False):\n",
    "    epsilon = 1e-4\n",
    "    if take_log10:\n",
    "        input_df[target_column] = np.log10(input_df[target_column] + epsilon)\n",
    "        \n",
    "    methods = [\"mean\", \"std\"]\n",
    "    df = input_df.groupby(group_key).agg({target_column: methods})\n",
    "    df.columns = df.columns.droplevel()\n",
    "    col = {method: target_column + \"_\" + method for method in methods}\n",
    "    df.rename(columns=col, inplace=True)\n",
    "    df_merge = input_df.merge(df.reset_index(), on=group_key)\n",
    "    df_merge[target_column + \"_norm_by_\" + group_key] = (\n",
    "        df_merge[target_column] - df_merge[target_column + \"_mean\"]\n",
    "    ) / df_merge[target_column + \"_std\"]\n",
    "    df_merge = df_merge.drop(labels=[col[\"mean\"], col[\"std\"]], axis=1)\n",
    "\n",
    "    gc.collect()\n",
    "    return df_merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_features(df, group_key=\"prop_id\", target_column=\"price_usd\", \n",
    "                       agg_methods=[\"mean\", \"median\", \"min\", \"max\"], \n",
    "                       transform_methods={\"mean\": [\"subtract\"]}):\n",
    "    grouped_data = df.groupby(group_key)[target_column].agg(agg_methods).reset_index()\n",
    "\n",
    "    for method in agg_methods:\n",
    "        old_name = method\n",
    "        new_name = f\"{method.upper()}_{target_column}_BY_{group_key}\"\n",
    "        grouped_data.rename(columns={old_name: new_name}, inplace=True)\n",
    "\n",
    "    df = df.merge(grouped_data, on=group_key, how='left')\n",
    "\n",
    "    for method, functions in transform_methods.items():\n",
    "        aggregated_col = f\"{method.upper()}_{target_column}_BY_{group_key}\"\n",
    "        for function in functions:\n",
    "            new_col_name = f\"{function.upper()}_{target_column}_{method.upper()}\"\n",
    "            if function == \"subtract\":\n",
    "                df[new_col_name] = df[target_column] - df[aggregated_col]\n",
    "            elif function == \"divide\":\n",
    "                df[new_col_name] = df[target_column] / df[aggregated_col]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\n",
    "    ('price_usd', 'srch_id', True),\n",
    "    ('price_usd', 'prop_id', False),\n",
    "    ('prop_starrating', 'srch_id', False),\n",
    "]\n",
    "\n",
    "aggregations = [\n",
    "    ('prop_id', 'price_usd'),\n",
    "    ('srch_id', 'prop_starrating'),\n",
    "    ('srch_id', 'prop_location_score1'),\n",
    "    ('srch_id', 'prop_location_score2'),\n",
    "    ('srch_id', 'prop_review_score'),\n",
    "    ('srch_id', 'promotion_flag'),\n",
    "    ('srch_destination_id', 'price_usd'),\n",
    "]\n",
    "\n",
    "operations2 = [\n",
    "    ('prop_starrating', 'srch_id', False),\n",
    "    ('prop_location_score1', 'srch_id', False),\n",
    "    ('prop_location_score2', 'srch_id', False),\n",
    "    ('prop_review_score', 'srch_id', False),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_column, group_key, log in operations:\n",
    "    df_test = normalize_features(df_test, group_key, target_column, log)\n",
    "\n",
    "aggregation_methods = ['mean']\n",
    "transformation_methods = {'mean': ['subtract']}\n",
    "\n",
    "for group_key, target_column in aggregations:\n",
    "    df_test = aggregate_features(\n",
    "        df_test, group_key, target_column, aggregation_methods, transformation_methods\n",
    "    )\n",
    "\n",
    "df_test.sort_values(by='srch_id', inplace=True)\n",
    "\n",
    "for target_column, group_key, log in operations2:\n",
    "    df_test = normalize_features(df_test, group_key, target_column, log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['position', 'click_bool', 'booking_bool', 'gross_bookings_usd', 'date_time']\n",
    "df.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "df_test.drop(columns=['date_time'], inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['srch_id', 'site_id', 'visitor_location_country_id',\n",
       "       'visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_country_id',\n",
       "       'prop_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
       "       'prop_location_score1', 'prop_location_score2',\n",
       "       'prop_log_historical_price', 'price_usd', 'promotion_flag',\n",
       "       'srch_destination_id', 'srch_length_of_stay', 'srch_booking_window',\n",
       "       'srch_adults_count', 'srch_children_count', 'srch_room_count',\n",
       "       'srch_saturday_night_bool', 'srch_query_affinity_score',\n",
       "       'orig_destination_distance', 'random_bool', 'comp1_rate_percent_diff',\n",
       "       'comp2_rate_percent_diff', 'comp3_rate_percent_diff',\n",
       "       'comp4_rate_percent_diff', 'comp5_rate_percent_diff',\n",
       "       'comp6_rate_percent_diff', 'comp7_rate_percent_diff',\n",
       "       'comp8_rate_percent_diff', 'month', 'day_of_week', 'is_weekend',\n",
       "       'positive_rate_count', 'negative_rate_count', 'positive_inv_count',\n",
       "       'negative_inv_count', 'positive_rate_diff_count',\n",
       "       'negative_rate_diff_count', 'avg_rate', 'avg_inv',\n",
       "       'avg_rate_percent_diff', 'location_score', 'star_rating_diff',\n",
       "       'price_diff', 'price_location_interaction', 'price_usd_norm_by_srch_id',\n",
       "       'price_usd_norm_by_prop_id', 'prop_starrating_norm_by_srch_id',\n",
       "       'MEAN_price_usd_BY_prop_id', 'SUBTRACT_price_usd_MEAN',\n",
       "       'MEAN_prop_starrating_BY_srch_id', 'SUBTRACT_prop_starrating_MEAN',\n",
       "       'MEAN_prop_location_score1_BY_srch_id',\n",
       "       'SUBTRACT_prop_location_score1_MEAN',\n",
       "       'MEAN_prop_location_score2_BY_srch_id',\n",
       "       'SUBTRACT_prop_location_score2_MEAN',\n",
       "       'MEAN_prop_review_score_BY_srch_id', 'SUBTRACT_prop_review_score_MEAN',\n",
       "       'MEAN_promotion_flag_BY_srch_id', 'SUBTRACT_promotion_flag_MEAN',\n",
       "       'MEAN_price_usd_BY_srch_destination_id',\n",
       "       'prop_location_score1_norm_by_srch_id',\n",
       "       'prop_location_score2_norm_by_srch_id',\n",
       "       'prop_review_score_norm_by_srch_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No HP optimization\n",
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # features = ['prop_starrating', 'prop_review_score', 'prop_location_score1', 'prop_location_score2', \n",
    "# #             'price_usd', 'promotion_flag', 'srch_length_of_stay', 'srch_booking_window', \n",
    "# #             'srch_adults_count', 'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool', \n",
    "# #             'orig_destination_distance', 'location_score', 'star_rating_diff', 'price_diff',\n",
    "# #             'price_location_interaction']\n",
    "# features = [\n",
    "#     'prop_starrating', 'prop_review_score', 'prop_location_score1', 'prop_location_score2', \n",
    "#     'price_usd', 'promotion_flag', 'srch_length_of_stay', 'srch_booking_window', \n",
    "#     'srch_adults_count', 'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool', \n",
    "#     'orig_destination_distance', 'location_score', 'star_rating_diff', 'price_diff',\n",
    "#     'price_location_interaction', 'avg_rate', 'avg_inv', 'avg_rate_percent_diff',\n",
    "#     'visitor_hist_starrating', 'visitor_hist_adr_usd', 'srch_query_affinity_score',\n",
    "#     'month', 'day_of_week', 'is_weekend'\n",
    "# ]\n",
    "# X = df[features]\n",
    "# y = df['target']\n",
    "\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "#                 max_depth = 5, alpha = 10, n_estimators = 100)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# df_test['predicted_score'] = model.predict(df_test[features]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "df_test = df_test.replace([np.inf, -np.inf], 0).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# features = [\n",
    "#     'prop_starrating', 'prop_review_score', 'prop_location_score1', 'prop_location_score2', \n",
    "#     'price_usd', 'promotion_flag', 'srch_length_of_stay', 'srch_booking_window', \n",
    "#     'srch_adults_count', 'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool', \n",
    "#     'orig_destination_distance', 'location_score', 'star_rating_diff', 'price_diff',\n",
    "#     'price_location_interaction', 'avg_rate', 'avg_inv', 'avg_rate_percent_diff',\n",
    "#     'visitor_hist_starrating', 'visitor_hist_adr_usd', 'srch_query_affinity_score',\n",
    "#     'month', 'day_of_week', 'is_weekend',\n",
    "#     'positive_rate_count', 'negative_rate_count', 'positive_inv_count', 'negative_inv_count', \n",
    "#     'positive_rate_diff_count', 'negative_rate_diff_count', \n",
    "#     'price_usd_norm_by_srch_id', 'price_usd_norm_by_prop_id', \n",
    "#     'prop_starrating_norm_by_srch_id', 'MEAN_price_usd_BY_prop_id', \n",
    "#     'SUBTRACT_price_usd_MEAN', 'MEAN_prop_starrating_BY_srch_id', \n",
    "#     'SUBTRACT_prop_starrating_MEAN', 'MEAN_prop_location_score1_BY_srch_id', \n",
    "#     'SUBTRACT_prop_location_score1_MEAN', 'MEAN_prop_location_score2_BY_srch_id', \n",
    "#     'SUBTRACT_prop_location_score2_MEAN', 'MEAN_prop_review_score_BY_srch_id', \n",
    "#     'SUBTRACT_prop_review_score_MEAN', 'MEAN_promotion_flag_BY_srch_id', \n",
    "#     'SUBTRACT_promotion_flag_MEAN', 'MEAN_price_usd_BY_srch_destination_id', \n",
    "#     'prop_location_score1_norm_by_srch_id', 'prop_location_score2_norm_by_srch_id', \n",
    "#     'prop_review_score_norm_by_srch_id'\n",
    "# ]\n",
    "\n",
    "# X = df[features]\n",
    "# y = df['target']\n",
    "\n",
    "# X_sample, _, y_sample, _ = train_test_split(X, y, test_size=0.9, random_state=69)\n",
    "\n",
    "# param_grid = {\n",
    "#     'colsample_bytree': [0.3, 0.6, 0.9],\n",
    "#     'learning_rate': [0.1, 0.01, 0.001],\n",
    "#     'max_depth': [3, 5, 7],\n",
    "#     'n_estimators': [50, 100, 200]\n",
    "# }\n",
    "\n",
    "# xgb_reg = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# grid_search = GridSearchCV(\n",
    "#     estimator=xgb_reg, \n",
    "#     param_grid=param_grid, \n",
    "#     cv=5, \n",
    "#     scoring='neg_mean_squared_error', \n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# grid_search.fit(X_sample, y_sample)\n",
    "\n",
    "# print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "# print(f\"Lowest RMSE found: {np.sqrt(np.abs(grid_search.best_score_))}\")\n",
    "\n",
    "# best_params = grid_search.best_params_\n",
    "# model = xgb.XGBRegressor(objective='reg:squarederror', **best_params)\n",
    "# model.fit(X, y)\n",
    "\n",
    "# df_test['predicted_score'] = model.predict(df_test[features])\n",
    "# y_pred = model.predict(X)\n",
    "# train_rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "# print(f\"Train RMSE: {train_rmse}\")\n",
    "\n",
    "# sorted_recommendations = df_test.sort_values(by=['srch_id', 'predicted_score'], ascending=[True, False])\n",
    "# sorted_recommendations[['srch_id', 'prop_id']].to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "all_features = [\n",
    "    'prop_starrating', 'prop_review_score', 'prop_location_score1', 'prop_location_score2', \n",
    "    'price_usd', 'promotion_flag', 'srch_length_of_stay', 'srch_booking_window', \n",
    "    'srch_adults_count', 'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool', \n",
    "    'orig_destination_distance', 'location_score', 'star_rating_diff', 'price_diff',\n",
    "    'price_location_interaction', 'avg_rate', 'avg_inv', 'avg_rate_percent_diff',\n",
    "    'visitor_hist_starrating', 'visitor_hist_adr_usd', 'srch_query_affinity_score',\n",
    "    'month', 'day_of_week', 'is_weekend',\n",
    "    'positive_rate_count', 'negative_rate_count', 'positive_inv_count', 'negative_inv_count', \n",
    "    'positive_rate_diff_count', 'negative_rate_diff_count', \n",
    "    'price_usd_norm_by_srch_id', 'price_usd_norm_by_prop_id', \n",
    "    'prop_starrating_norm_by_srch_id', 'MEAN_price_usd_BY_prop_id', \n",
    "    'SUBTRACT_price_usd_MEAN', 'MEAN_prop_starrating_BY_srch_id', \n",
    "    'SUBTRACT_prop_starrating_MEAN', 'MEAN_prop_location_score1_BY_srch_id', \n",
    "    'SUBTRACT_prop_location_score1_MEAN', 'MEAN_prop_location_score2_BY_srch_id', \n",
    "    'SUBTRACT_prop_location_score2_MEAN', 'MEAN_prop_review_score_BY_srch_id', \n",
    "    'SUBTRACT_prop_review_score_MEAN', 'MEAN_promotion_flag_BY_srch_id', \n",
    "    'SUBTRACT_promotion_flag_MEAN', 'MEAN_price_usd_BY_srch_destination_id', \n",
    "    'prop_location_score1_norm_by_srch_id', 'prop_location_score2_norm_by_srch_id', \n",
    "    'prop_review_score_norm_by_srch_id'\n",
    "]\n",
    "\n",
    "\n",
    "available_features = [feat for feat in all_features if feat in df.columns]\n",
    "\n",
    "\n",
    "X = df[available_features]\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "X_sample, _, y_sample, _ = train_test_split(X, y, test_size=0.9, random_state=69)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.6, 0.9],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_reg, \n",
    "    param_grid=param_grid, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_sample, y_sample)\n",
    "\n",
    "\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Lowest RMSE found: {np.sqrt(np.abs(grid_search.best_score_))}\")\n",
    "\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', **best_params)\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "df_test['predicted_score'] = model.predict(df_test[available_features])\n",
    "y_pred = model.predict(X)\n",
    "train_rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "print(f\"Train RMSE: {train_rmse}\")\n",
    "\n",
    "\n",
    "sorted_recommendations = df_test.sort_values(by=['srch_id', 'predicted_score'], ascending=[True, False])\n",
    "sorted_recommendations[['srch_id', 'prop_id']].to_csv('submissions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "data_csr = csr_matrix((df['target'], (df['srch_id'], df['prop_id'])))\n",
    "\n",
    "svd = TruncatedSVD(n_components=200, random_state=69)\n",
    "user_factors = svd.fit_transform(data_csr)\n",
    "prop_factors = svd.components_.T\n",
    "\n",
    "user_vector_shape = user_factors[0].shape\n",
    "zero_vector = np.zeros(user_vector_shape, dtype=user_factors.dtype)\n",
    "\n",
    "user_factors = np.vstack([user_factors, zero_vector, zero_vector])\n",
    "\n",
    "df_test['predicted_score'] = model.predict(df_test[features])\n",
    "\n",
    "final_scores = []\n",
    "for user_id, prop_id, predicted_score in zip(df_test['srch_id'], df_test['prop_id'], df_test['predicted_score']):\n",
    "    user_vector = user_factors[user_id] if user_id < len(user_factors) else zero_vector\n",
    "    prop_vector = prop_factors[prop_id] if prop_id < len(prop_factors) else zero_vector\n",
    "    svd_score = np.dot(user_vector, prop_vector)\n",
    "    final_score = predicted_score + 0.1 * svd_score\n",
    "    final_scores.append(final_score)\n",
    "\n",
    "df_test['final_score'] = final_scores\n",
    "sorted_recommendations = df_test.sort_values(by=['srch_id', 'final_score'], ascending=[True, False])\n",
    "\n",
    "sorted_recommendations[['srch_id', 'prop_id']].to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
